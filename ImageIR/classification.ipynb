{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>> Minibatch [7/28], Loss: 1.9670, Accuracy: 0.2746\n",
      ">>>>> Minibatch [14/28], Loss: 1.8444, Accuracy: 0.3460\n",
      ">>>>> Minibatch [21/28], Loss: 1.7448, Accuracy: 0.4048\n",
      ">>>>> Minibatch [28/28], Loss: 1.6486, Accuracy: 0.4543\n",
      "Train [1/10], Loss: 1.6486, Accuracy: 0.4543\n",
      ">>>>> Minibatch [7/12], Loss: 1.2614, Accuracy: 0.6116\n",
      "Validation [1/10], Loss: 1.2720, Accuracy: 0.6263\n",
      ">>>>> Minibatch [7/28], Loss: 1.1736, Accuracy: 0.7031\n",
      ">>>>> Minibatch [14/28], Loss: 1.1371, Accuracy: 0.6987\n",
      ">>>>> Minibatch [21/28], Loss: 1.0863, Accuracy: 0.7098\n",
      ">>>>> Minibatch [28/28], Loss: 1.0587, Accuracy: 0.7158\n",
      "Train [2/10], Loss: 1.0587, Accuracy: 0.7158\n",
      ">>>>> Minibatch [7/12], Loss: 0.9714, Accuracy: 0.7098\n",
      "Validation [2/10], Loss: 0.9736, Accuracy: 0.7164\n",
      ">>>>> Minibatch [7/28], Loss: 0.8498, Accuracy: 0.7991\n",
      ">>>>> Minibatch [14/28], Loss: 0.8452, Accuracy: 0.7857\n",
      ">>>>> Minibatch [21/28], Loss: 0.8439, Accuracy: 0.7842\n",
      ">>>>> Minibatch [28/28], Loss: 0.8221, Accuracy: 0.7900\n",
      "Train [3/10], Loss: 0.8221, Accuracy: 0.7900\n",
      ">>>>> Minibatch [7/12], Loss: 0.8637, Accuracy: 0.7433\n",
      "Validation [3/10], Loss: 0.8372, Accuracy: 0.7567\n",
      ">>>>> Minibatch [7/28], Loss: 0.7133, Accuracy: 0.8147\n",
      ">>>>> Minibatch [14/28], Loss: 0.7304, Accuracy: 0.7980\n",
      ">>>>> Minibatch [21/28], Loss: 0.7420, Accuracy: 0.7969\n",
      ">>>>> Minibatch [28/28], Loss: 0.7238, Accuracy: 0.7962\n",
      "Train [4/10], Loss: 0.7238, Accuracy: 0.7962\n",
      ">>>>> Minibatch [7/12], Loss: 0.8381, Accuracy: 0.7254\n",
      "Validation [4/10], Loss: 0.7826, Accuracy: 0.7594\n",
      ">>>>> Minibatch [7/28], Loss: 0.6430, Accuracy: 0.8147\n",
      ">>>>> Minibatch [14/28], Loss: 0.6413, Accuracy: 0.8103\n",
      ">>>>> Minibatch [21/28], Loss: 0.6373, Accuracy: 0.8155\n",
      ">>>>> Minibatch [28/28], Loss: 0.6276, Accuracy: 0.8174\n",
      "Train [5/10], Loss: 0.6276, Accuracy: 0.8174\n",
      ">>>>> Minibatch [7/12], Loss: 0.7276, Accuracy: 0.7500\n",
      "Validation [5/10], Loss: 0.7418, Accuracy: 0.7634\n",
      ">>>>> Minibatch [7/28], Loss: 0.5745, Accuracy: 0.8348\n",
      ">>>>> Minibatch [14/28], Loss: 0.5710, Accuracy: 0.8382\n",
      ">>>>> Minibatch [21/28], Loss: 0.5686, Accuracy: 0.8393\n",
      ">>>>> Minibatch [28/28], Loss: 0.5828, Accuracy: 0.8316\n",
      "Train [6/10], Loss: 0.5828, Accuracy: 0.8316\n",
      ">>>>> Minibatch [7/12], Loss: 0.6985, Accuracy: 0.7656\n",
      "Validation [6/10], Loss: 0.6992, Accuracy: 0.7796\n",
      ">>>>> Minibatch [7/28], Loss: 0.5802, Accuracy: 0.8438\n",
      ">>>>> Minibatch [14/28], Loss: 0.5606, Accuracy: 0.8438\n",
      ">>>>> Minibatch [21/28], Loss: 0.5358, Accuracy: 0.8542\n",
      ">>>>> Minibatch [28/28], Loss: 0.5423, Accuracy: 0.8487\n",
      "Train [7/10], Loss: 0.5423, Accuracy: 0.8487\n",
      ">>>>> Minibatch [7/12], Loss: 0.6483, Accuracy: 0.7768\n",
      "Validation [7/10], Loss: 0.6690, Accuracy: 0.7809\n",
      ">>>>> Minibatch [7/28], Loss: 0.5421, Accuracy: 0.8326\n",
      ">>>>> Minibatch [14/28], Loss: 0.5413, Accuracy: 0.8371\n",
      ">>>>> Minibatch [21/28], Loss: 0.5177, Accuracy: 0.8497\n",
      ">>>>> Minibatch [28/28], Loss: 0.5195, Accuracy: 0.8470\n",
      "Train [8/10], Loss: 0.5195, Accuracy: 0.8470\n",
      ">>>>> Minibatch [7/12], Loss: 0.6642, Accuracy: 0.7768\n",
      "Validation [8/10], Loss: 0.6561, Accuracy: 0.7890\n",
      ">>>>> Minibatch [7/28], Loss: 0.4841, Accuracy: 0.8616\n",
      ">>>>> Minibatch [14/28], Loss: 0.4857, Accuracy: 0.8527\n",
      ">>>>> Minibatch [21/28], Loss: 0.4859, Accuracy: 0.8579\n",
      ">>>>> Minibatch [28/28], Loss: 0.4810, Accuracy: 0.8607\n",
      "Train [9/10], Loss: 0.4810, Accuracy: 0.8607\n",
      ">>>>> Minibatch [7/12], Loss: 0.6248, Accuracy: 0.7902\n",
      "Validation [9/10], Loss: 0.6458, Accuracy: 0.7944\n",
      ">>>>> Minibatch [7/28], Loss: 0.4399, Accuracy: 0.8906\n",
      ">>>>> Minibatch [14/28], Loss: 0.4512, Accuracy: 0.8761\n",
      ">>>>> Minibatch [21/28], Loss: 0.4430, Accuracy: 0.8765\n",
      ">>>>> Minibatch [28/28], Loss: 0.4657, Accuracy: 0.8659\n",
      "Train [10/10], Loss: 0.4657, Accuracy: 0.8659\n",
      ">>>>> Minibatch [7/12], Loss: 0.6156, Accuracy: 0.7991\n",
      "Validation [10/10], Loss: 0.6451, Accuracy: 0.7957\n",
      "Fine-tuning finished.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "class CustomResNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CustomResNet, self).__init__()\n",
    "        self.resnet = resnet18(pretrained=True)\n",
    "        for param in self.resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "        num_ftrs = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  \n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data = ImageFolder(root=data_dir, transform=transform)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "train_dir = '../Image Data/Paintings/training'\n",
    "val_dir = '../Image Data/Paintings/testing'\n",
    "train_dataset = CustomDataset(train_dir, transform=transform)\n",
    "val_dataset = CustomDataset(val_dir, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "num_classes = 8\n",
    "model = CustomResNet(num_classes)\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 10\n",
    "print_every = 7 \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    minibatch_counter = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        minibatch_counter += 1\n",
    "        if minibatch_counter % print_every == 0:\n",
    "            minibatch_loss = running_loss / total\n",
    "            minibatch_accuracy = correct / total\n",
    "            print(f'>>>>> Minibatch [{minibatch_counter}/{len(train_loader)}], Loss: {minibatch_loss:.4f}, Accuracy: {minibatch_accuracy:.4f}')\n",
    "\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    epoch_accuracy = correct / total\n",
    "    print(f'Train [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}')\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    minibatch_counter = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            minibatch_counter += 1\n",
    "            if minibatch_counter % print_every == 0:\n",
    "                minibatch_loss = val_loss / val_total\n",
    "                minibatch_accuracy = val_correct / val_total\n",
    "                print(f'>>>>> Minibatch [{minibatch_counter}/{len(val_loader)}], Loss: {minibatch_loss:.4f}, Accuracy: {minibatch_accuracy:.4f}')\n",
    "\n",
    "    val_epoch_loss = val_loss / len(val_dataset)\n",
    "    val_epoch_accuracy = val_correct / val_total\n",
    "    print(f'Validation [{epoch+1}/{num_epochs}], Loss: {val_epoch_loss:.4f}, Accuracy: {val_epoch_accuracy:.4f}')\n",
    "\n",
    "print('Fine-tuning finished.')\n",
    "torch.save(model.state_dict(), 'Painting.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: ../Image Data/Paintings/training/Kangra/p1.jpg, Predicted Class: Kangra\n",
      "Image: ../Image Data/Paintings/training/Madhubani/p1.jpg, Predicted Class: Madhubani\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import resnet18\n",
    "from PIL import Image\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  \n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "num_classes = 8  \n",
    "model = CustomResNet(num_classes)\n",
    "model.load_state_dict(torch.load('Painting.pth'))\n",
    "model.eval()\n",
    "\n",
    "train_dataset = ImageFolder(root='../Image Data/Paintings/training', transform=transform)\n",
    "class_names = train_dataset.classes\n",
    "def predict_image(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    image = transform(image).unsqueeze(0)  \n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "    _, predicted = torch.max(output, 1)\n",
    "    predicted_class = class_names[predicted.item()] \n",
    "    return predicted_class\n",
    "\n",
    "image_paths = ['../Image Data/Paintings/training/Kangra/p1.jpg', '../Image Data/Paintings/training/Madhubani/p1.jpg']  \n",
    "for image_path in image_paths:\n",
    "    predicted_class = predict_image(image_path)\n",
    "    print(f'Image: {image_path}, Predicted Class: {predicted_class}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit ('3.9.10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3c9c1cb92ab039c53a0f0c6a3a1c445ea784bd0a545c5bb81dae74e9464ebef1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
